{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark demo\n",
    "\n",
    "Illustrates how to use the ammonite-spark library, that adds Spark support to (my [fork](https://github.com/alexarchambault/ammonite-shell) of) the [Ammonite](https://github.com/lihaoyi/Ammonite/) REPL or a notebook.\n",
    "\n",
    "**Just a proof-of-concept of things that can be done.**\n",
    "\n",
    "All of this was tested (and is unit tested, although not on Travis, see [1](https://github.com/alexarchambault/jupyter-scala/blob/master/kernel/src/test/scala/jupyter/scala/LocalSparkTests.scala), [2](https://github.com/alexarchambault/jupyter-scala/blob/master/kernel/src/test/scala/jupyter/scala/LocalClusterSparkTests.scala), [3](https://github.com/alexarchambault/jupyter-scala/blob/master/kernel/src/test/scala/jupyter/scala/StandAloneClusterSparkTests.scala)):\n",
    "* locally (master like `local`),\n",
    "* on a local cluster (`local-cluster[1,1,512]`), and\n",
    "* on a (docker based :-|) standalone cluster (`spark://master:7077`).\n",
    "\n",
    "This would deserve more large scale tests (ec2, ...). Use with caution. Nothing specific was done for YARN, I don't expect YARN clusters to work as is.\n",
    "\n",
    "The examples come mostly from the spark-repl test suite: https://github.com/apache/spark/blob/master/repl/scala-2.11/src/test/scala/org/apache/spark/repl/ReplSuite.scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from a bare scala 2.10 jupyter scala notebook. First, let's fetch ammonite-spark.\n",
    "\n",
    "Notice the `1.3` in the dependency name, which corresponds to our spark version, and the `2.10.5` which is the full current scala version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::: WARNINGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: problems summary ::"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Wed May 20 03:09:16 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Wed May 20 01:57:50 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri May 15 16:53:44 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-spark_1.3_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Wed May 20 01:58:01 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-spark_1.3_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "om.github.alexarchambault#ammonite-shell-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Wed May 20 01:57:55 CEST 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-shell-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.ivy(\"com.github.alexarchambault\" % \"ammonite-spark_1.3_2.10.5\" % \"0.3.1-SNAPSHOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a handle, of type `ammonite.spark.Spark`, which is able to grab info from the current interpreter, and uses it later to initialize spark.\n",
    "\n",
    "Note the `@transient` annotation added to it, so that it will not get serialized when running closures on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mSpark\u001b[0m: \u001b[32mammonite.spark.Spark\u001b[0m = Spark(uninitialized)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@transient val Spark = new ammonite.spark.Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `SparkContext` is accessible through the `sc` method of the spark handle. It is *lazily* initialized, which means it is not yet, as we didn't call the `sc` method.\n",
    "\n",
    "Before that, we'll setup the spark config, through a `SparkConf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Spark.withConf(_\n",
    "  .setMaster(\"spark://master:7077\")\n",
    "  .set(\"spark.home\", \"/path-to-spark-distrib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mSpark.sc\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Spark.sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/05/20 03:16:07 INFO Spark$$anon$2: Running Spark version 1.3.1\n",
      "15/05/20 03:16:07 WARN Utils: Your hostname, pc-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.0.15 instead (on interface eth0)\n",
      "15/05/20 03:16:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "15/05/20 03:16:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/05/20 03:16:07 INFO SecurityManager: Changing view acls to: alexandre\n",
      "15/05/20 03:16:07 INFO SecurityManager: Changing modify acls to: alexandre\n",
      "15/05/20 03:16:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(alexandre); users with modify permissions: Set(alexandre)\n",
      "15/05/20 03:16:07 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/05/20 03:16:07 INFO Remoting: Starting remoting\n",
      "15/05/20 03:16:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.15:41915]\n",
      "15/05/20 03:16:08 INFO Utils: Successfully started service 'sparkDriver' on port 41915.\n",
      "15/05/20 03:16:08 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/05/20 03:16:08 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/05/20 03:16:08 INFO DiskBlockManager: Created local directory at /tmp/spark-baae041d-5129-4485-837e-fa58294c240d/blockmgr-039bda43-a58f-4911-ad73-c680fc4944a4\n",
      "15/05/20 03:16:08 INFO MemoryStore: MemoryStore started with capacity 943.9 MB\n",
      "15/05/20 03:16:08 INFO HttpFileServer: HTTP File server directory is /tmp/spark-bbcedcdc-b95c-4115-b18c-32c999f7fcb8/httpd-f66e091a-da5f-427d-8e7d-0f3d7d1b6757\n",
      "15/05/20 03:16:08 INFO HttpServer: Starting HTTP Server\n",
      "15/05/20 03:16:08 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/05/20 03:16:08 INFO AbstractConnector: Started SocketConnector@0.0.0.0:33288\n",
      "15/05/20 03:16:08 INFO Utils: Successfully started service 'HTTP file server' on port 33288.\n",
      "15/05/20 03:16:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/05/20 03:16:13 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/05/20 03:16:13 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n",
      "15/05/20 03:16:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "15/05/20 03:16:13 INFO SparkUI: Started SparkUI at http://192.168.0.15:4040\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ammonite-api_2.10.5-0.3.1-SNAPSHOT.jar at http://192.168.0.15:33288/jars/ammonite-api_2.10.5-0.3.1-SNAPSHOT.jar with timestamp 1432084573380\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-api_2.10-0.2.0-SNAPSHOT.jar at http://192.168.0.15:33288/jars/jupyter-api_2.10-0.2.0-SNAPSHOT.jar with timestamp 1432084573381\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-scala-api_2.10.5-0.2.0-SNAPSHOT.jar at http://192.168.0.15:33288/jars/jupyter-scala-api_2.10.5-0.2.0-SNAPSHOT.jar with timestamp 1432084573381\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ammonite-pprint_2.10-0.3.0.jar at http://192.168.0.15:33288/jars/ammonite-pprint_2.10-0.3.0.jar with timestamp 1432084573381\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-library-2.10.5.jar at http://192.168.0.15:33288/jars/scala-library-2.10.5.jar with timestamp 1432084573459\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-reflect-2.10.5.jar at http://192.168.0.15:33288/jars/scala-reflect-2.10.5.jar with timestamp 1432084573462\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/com.github.alexarchambault/ammonite-spark_1.3_2.10.5/jars/ammonite-spark_1.3_2.10.5-0.3.1-SNAPSHOT.jar at http://192.168.0.15:33288/jars/ammonite-spark_1.3_2.10.5-0.3.1-SNAPSHOT.jar with timestamp 1432084573462\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/com.github.alexarchambault/ammonite-shell-api_2.10.5/jars/ammonite-shell-api_2.10.5-0.3.1-SNAPSHOT.jar at http://192.168.0.15:33288/jars/ammonite-shell-api_2.10.5-0.3.1-SNAPSHOT.jar with timestamp 1432084573462\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-server/jars/jetty-server-8.1.14.v20131031.jar at http://192.168.0.15:33288/jars/jetty-server-8.1.14.v20131031.jar with timestamp 1432084573555\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-continuation/jars/jetty-continuation-8.1.14.v20131031.jar at http://192.168.0.15:33288/jars/jetty-continuation-8.1.14.v20131031.jar with timestamp 1432084573555\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-http/jars/jetty-http-8.1.14.v20131031.jar at http://192.168.0.15:33288/jars/jetty-http-8.1.14.v20131031.jar with timestamp 1432084573556\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-io/jars/jetty-io-8.1.14.v20131031.jar at http://192.168.0.15:33288/jars/jetty-io-8.1.14.v20131031.jar with timestamp 1432084573556\n",
      "15/05/20 03:16:13 INFO Spark$$anon$2: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-util/jars/jetty-util-8.1.14.v20131031.jar at http://192.168.0.15:33288/jars/jetty-util-8.1.14.v20131031.jar with timestamp 1432084573556\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@master:7077/user/Master...\n",
      "15/05/20 03:16:13 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150520011613-0001\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor added: app-20150520011613-0001/0 on worker-20150519234854-slave1-7078 (slave1:7078) with 8 cores\n",
      "15/05/20 03:16:13 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150520011613-0001/0 on hostPort slave1:7078 with 8 cores, 512.0 MB RAM\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor added: app-20150520011613-0001/1 on worker-20150519234855-slave3-7078 (slave3:7078) with 8 cores\n",
      "15/05/20 03:16:13 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150520011613-0001/1 on hostPort slave3:7078 with 8 cores, 512.0 MB RAM\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor added: app-20150520011613-0001/2 on worker-20150519234855-slave2-7078 (slave2:7078) with 8 cores\n",
      "15/05/20 03:16:13 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150520011613-0001/2 on hostPort slave2:7078 with 8 cores, 512.0 MB RAM\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor updated: app-20150520011613-0001/0 is now LOADING\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor updated: app-20150520011613-0001/1 is now LOADING\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor updated: app-20150520011613-0001/2 is now LOADING\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor updated: app-20150520011613-0001/0 is now RUNNING\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor updated: app-20150520011613-0001/1 is now RUNNING\n",
      "15/05/20 03:16:13 INFO AppClient$ClientActor: Executor updated: app-20150520011613-0001/2 is now RUNNING\n",
      "15/05/20 03:16:14 INFO NettyBlockTransferService: Server created on 51780\n",
      "15/05/20 03:16:14 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/05/20 03:16:14 INFO BlockManagerMasterActor: Registering block manager 192.168.0.15:51780 with 943.9 MB RAM, BlockManagerId(<driver>, 192.168.0.15, 51780)\n",
      "15/05/20 03:16:14 INFO BlockManagerMaster: Registered BlockManager\n",
      "15/05/20 03:16:14 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Spark.start() // equivalent to just calling sc, triggers its initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maccum\u001b[0m: \u001b[32morg.apache.spark.Accumulator[Int]\u001b[0m = 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).foreach(x => accum += x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m55\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mdouble\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def double(x: Int) = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m110\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => double(x)).collect().reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutable variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mv\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m7\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var v = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mgetV\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getV() = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m70\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => getV()).collect().reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres17\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m100\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => getV()).collect().reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining your own classes is ok\n",
    "\n",
    "and more complex definions should too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mSum\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Sum(exp: String, exp2: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36ma\u001b[0m: \u001b[32mcmd19.INSTANCE.$ref$cmd18.Sum\u001b[0m = \u001b[33mSum\u001b[0m(\u001b[32m\"A\"\u001b[0m, \u001b[32m\"B\"\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val a = Sum(\"A\", \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mb\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def b(a: Sum): String = a match { case Sum(_, _) => \"Found Sum\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"Found Sum\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL :|\n",
    "\n",
    "Spark SQL doesn't work from here because of https://issues.apache.org/jira/browse/SPARK-5281. It should be fixed in the next Spark releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mSpark.sqlContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36msqlContext.implicits._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Spark.sqlContext\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTestCaseClass\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class TestCaseClass(value: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "scala.reflect.internal.MissingRequirementError: class org.apache.spark.sql.catalyst.ScalaReflection in JavaMirror with sun.misc.Launcher$AppClassLoader@6e0be858 of type class sun.misc.Launcher$AppClassLoader with classpath [file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-logging-slf4j_2.10-2.1.2.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/test-interface-1.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/monocle-macro_2.10-0.5.1.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-logging-api_2.10-2.1.2.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jeromq-0.3.4.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/quasiquotes_2.10-2.0.1.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-reflect-2.10.5.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/paradise_2.10.5-2.0.1.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scodec-bits_2.10-1.0.4.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/logback-classic-1.0.13.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/case-app_2.10-0.2.2.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ammonite-interpreter_2.10.5-0.3.1-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scalaz-core_2.10-7.1.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-scala_2.10.5-0.2.0-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-parser_2.10-0.1.4.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-scala-cli_2.10.5-0.2.0-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ivy-light_2.10.5-0.3.1-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-kernel_2.10-0.2.0-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/shapeless_2.10.4-2.1.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-api_2.10-0.2.0-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/argonaut-shapeless_6.1_2.10-0.1.1.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/slf4j-api-1.7.7.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-compiler-2.10.5.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/utest_2.10-0.3.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/config-1.2.1.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/argonaut_2.10-6.1-M5.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/parboiled_2.10-2.1.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/jupyter-scala-api_2.10.5-0.2.0-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scala-library-2.10.5.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scalaz-stream_2.10-0.6a.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/logback-core-1.0.13.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ammonite-pprint_2.10-0.3.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scalaz-effect_2.10-7.1.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ivy-2.4.0.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/ammonite-api_2.10.5-0.3.1-SNAPSHOT.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/monocle-core_2.10-0.5.1.jar,file:/home/alexandre/projects/jupyter-scala/cli/target/pack/lib/scalaz-concurrent_2.10-7.1.0.jar] and parent being sun.misc.Launcher$ExtClassLoader@53045c6c of type class sun.misc.Launcher$ExtClassLoader with classpath [file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar] and parent being primordial classloader with boot classpath [/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/classes] not found.",
      "\tscala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:16)",
      "\tscala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:17)",
      "\tscala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)",
      "\tscala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:61)",
      "\tscala.reflect.internal.Mirrors$RootsBase.staticModuleOrClass(Mirrors.scala:72)",
      "\tscala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:119)",
      "\tscala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:21)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$$typecreator1$1.apply(ScalaReflection.scala:127)",
      "\tscala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:231)",
      "\tscala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:231)",
      "\tscala.reflect.api.TypeTags$class.typeOf(TypeTags.scala:335)",
      "\tscala.reflect.api.Universe.typeOf(Universe.scala:59)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:127)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:30)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:112)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:30)",
      "\torg.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:320)",
      "\torg.apache.spark.sql.SQLContext$implicits$.rddToDataFrameHolder(SQLContext.scala:258)"
     ]
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => TestCaseClass(x)).toDF().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused non serializable things are fine\n",
    "\n",
    "If they are not used in parallel calculations, they do not prevent serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTestClass\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TestClass() { def testMethod = 3; override def toString = \"TestClass\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:41: not found: type TestClass",
      "                val t = new TestClass",
      "                            ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "// not serializable\n",
    "val t = new TestClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mt.testMethod\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import t.testMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTestCaseClass\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// serializable\n",
    "case class TestCaseClass(value: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres32\u001b[0m: \u001b[32mscala.Array[cmd32.INSTANCE.$ref$cmd31.TestCaseClass]\u001b[0m = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m1\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m2\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m3\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m4\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m5\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m6\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m7\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m8\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m9\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m10\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// some parallel calculations with the serializable class\n",
    "sc.parallelize(1 to 10).map(x => TestCaseClass(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** jupyter-scala: handle fully cross-versioned artifact with a smoother syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** jupyter-scala: interpreter API to add config files to the classpath (e.g. log4j config below)\n",
    "\n",
    "**TODO** jupyter-scala: is the `@transient` still necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** jupyter-scala: do we still have to be cautious not to return the SparkConf? It should be ok now that we filter imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** jupyter-spark: do like spark with hostname, do not use a loopback address"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
